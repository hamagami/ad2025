{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMffDwMLTF/N6E+boFUwrJl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamagami/ad2025/blob/main/PatchCore_timm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PatchCore"
      ],
      "metadata": {
        "id": "hTmErCEqcmO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# セル1: MVTec AD を mydrive 直リンクからダウンロード＆展開（Drive不要）\n",
        "# - TARGET=\"all\" なら全体 (~5GB)\n",
        "# - それ以外はクラス名（例: \"bottle\"）のみをダウンロード\n",
        "# - 展開先: /content/mvtec_anomaly_detection/<class>/...\n",
        "# - 互換リンク: /content/mvtec -> /content/mvtec_anomaly_detection\n",
        "# =====================================================\n",
        "\n",
        "import os, subprocess, tarfile, zipfile, glob\n",
        "\n",
        "TARGET = \"bottle\"  # \"all\" または 15クラス名のいずれか: bottle,cable,capsule,carpet,grid,hazelnut,leather,metal_nut,pill,screw,tile,toothbrush,transistor,wood,zipper\n",
        "DEST_PARENT = \"/content\"\n",
        "DEST_ROOT   = os.path.join(DEST_PARENT, \"mvtec_anomaly_detection\")\n",
        "COMP_LINK   = \"/content/mvtec\"  # 互換リンク\n",
        "\n",
        "WHOLE_URL = \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938113-1629952094/mvtec_anomaly_detection.tar.xz\"\n",
        "CLASS_URLS = {\n",
        "    \"bottle\":      \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937370-1629951468/bottle.tar.xz\",\n",
        "    \"cable\":       \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937413-1629951498/cable.tar.xz\",\n",
        "    \"capsule\":     \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937454-1629951595/capsule.tar.xz\",\n",
        "    \"carpet\":      \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937484-1629951672/carpet.tar.xz\",\n",
        "    \"grid\":        \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937487-1629951814/grid.tar.xz\",\n",
        "    \"hazelnut\":    \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937545-1629951845/hazelnut.tar.xz\",\n",
        "    \"leather\":     \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937607-1629951964/leather.tar.xz\",\n",
        "    \"metal_nut\":   \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937637-1629952063/metal_nut.tar.xz\",\n",
        "    \"pill\":        \"https://www.mydrive.ch/shares/43421/11a215a5749fcfb75e331ddd5f8e43ee/download/420938129-1629953099/pill.tar.xz\",\n",
        "    \"screw\":       \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938130-1629953152/screw.tar.xz\",\n",
        "    \"tile\":        \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938133-1629953189/tile.tar.xz\",\n",
        "    \"toothbrush\":  \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938134-1629953256/toothbrush.tar.xz\",\n",
        "    \"transistor\":  \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938166-1629953277/transistor.tar.xz\",\n",
        "    \"wood\":        \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938383-1629953354/wood.tar.xz\",\n",
        "    \"zipper\":      \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938385-1629953449/zipper.tar.xz\",\n",
        "}\n",
        "IMG_EXTS = ('*.png','*.jpg','*.jpeg','*.bmp','*.tif','*.tiff')\n",
        "\n",
        "def run(cmd):\n",
        "    print(\"[RUN]\", cmd)\n",
        "    rc = subprocess.call(cmd, shell=True)\n",
        "    if rc != 0:\n",
        "        raise RuntimeError(f\"Command failed (rc={rc}): {cmd}\")\n",
        "\n",
        "def list_images_multi_ext(dirpath: str):\n",
        "    files=[]\n",
        "    for ext in IMG_EXTS:\n",
        "        files += glob.glob(os.path.join(dirpath, '**', ext), recursive=True)\n",
        "    return sorted(files)\n",
        "\n",
        "os.makedirs(DEST_ROOT, exist_ok=True)\n",
        "\n",
        "if TARGET == \"all\":\n",
        "    tar_path = os.path.join(DEST_PARENT, \"mvtec_ad.tar.xz\")\n",
        "    run(f'wget --progress=dot:mega -O \"{tar_path}\" \"{WHOLE_URL}\"')\n",
        "    run(f'tar -xf \"{tar_path}\" -C \"{DEST_PARENT}\"')\n",
        "else:\n",
        "    if TARGET not in CLASS_URLS:\n",
        "        raise ValueError(f\"Unknown TARGET='{TARGET}'.\")\n",
        "    tar_path = os.path.join(DEST_PARENT, f\"{TARGET}.tar.xz\")\n",
        "    run(f'wget --progress=dot:mega -O \"{tar_path}\" \"{CLASS_URLS[TARGET]}\"')\n",
        "    run(f'tar -xf \"{tar_path}\" -C \"{DEST_ROOT}\"')\n",
        "\n",
        "# 互換リンク（/content/mvtec）が無ければ作成\n",
        "if not os.path.exists(COMP_LINK):\n",
        "    try:\n",
        "        os.symlink(DEST_ROOT, COMP_LINK)\n",
        "        print(\"[INFO] Symlink:\", COMP_LINK, \"->\", DEST_ROOT)\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] symlink failed:\", e)\n",
        "\n",
        "# 簡易チェック＆要約\n",
        "def quick_summary(root_parent=\"/content\"):\n",
        "    candidates = [\"/content/mvtec\", \"/content/mvtec_anomaly_detection\"]\n",
        "    ok = False\n",
        "    for base in candidates:\n",
        "        if not os.path.isdir(base): continue\n",
        "        classes = [d for d in os.listdir(base) if os.path.isdir(os.path.join(base, d))]\n",
        "        classes = sorted([c for c in classes if os.path.isdir(os.path.join(base,c,\"test\")) or os.path.isdir(os.path.join(base,c,\"train\"))])\n",
        "        if not classes: continue\n",
        "        print(f\"[READY] Found classes under {base}: {classes[:10]}{' ...' if len(classes)>10 else ''}\")\n",
        "        # 1クラスだけ簡易枚数\n",
        "        cls0 = classes[0]\n",
        "        tg = os.path.join(base, cls0, \"train\", \"good\")\n",
        "        ttg= os.path.join(base, cls0, \"test\",  \"good\")\n",
        "        n_tg = len(list_images_multi_ext(tg)) if os.path.isdir(tg) else 0\n",
        "        n_ttg= len(list_images_multi_ext(ttg)) if os.path.isdir(ttg) else 0\n",
        "        print(f\"[SUMMARY] sample class={cls0}: train/good={n_tg}, test/good={n_ttg}\")\n",
        "        ok = True\n",
        "    if not ok:\n",
        "        raise FileNotFoundError(\"Extraction finished but dataset layout not detected. Please check archive.\")\n",
        "    print(\"[DONE] MVTec AD is ready under /content/mvtec_anomaly_detection\")\n",
        "\n",
        "quick_summary()\n"
      ],
      "metadata": {
        "id": "Dn0zg-RdZW3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# セル2: PatchCore (timm/ResNet18, layer2+layer3, MiniBatchKMeans coreset)\n",
        "# - /content/mvtec_anomaly_detection/<class>/... を自動検出（セル1準拠）\n",
        "# - 学習: 正常パッチ埋め込み → コアセット（MiniBatchKMeansで代表ベクトル）→ 1-NN 距離\n",
        "# - 推論: 位置スコアマップ（距離） → 上位p%平均で画像スコア\n",
        "# - 閾値: train/good を split した val の分位点\n",
        "# - 可視化: ROC/PR, Top異常, 誤検出/見逃し\n",
        "# ============================================================\n",
        "\n",
        "import os, glob, math, random\n",
        "from typing import List, Tuple, Dict\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# timm / sklearn が無ければ入れる\n",
        "try:\n",
        "    import timm\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"timm\"], check=False)\n",
        "    import timm\n",
        "\n",
        "try:\n",
        "    from sklearn.neighbors import NearestNeighbors\n",
        "    from sklearn.cluster import MiniBatchKMeans\n",
        "    from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, average_precision_score\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"scikit-learn\"], check=False)\n",
        "    from sklearn.neighbors import NearestNeighbors\n",
        "    from sklearn.cluster import MiniBatchKMeans\n",
        "    from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, average_precision_score\n",
        "\n",
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "# ---------------- Config ----------------\n",
        "CLASS_NAME       = globals().get(\"TARGET\", \"bottle\")   # セル1の TARGET に合わせる\n",
        "ROOT_CANDIDATES  = [\"/content/mvtec\", \"/content/mvtec_anomaly_detection\"]\n",
        "\n",
        "IMG_SIZE         = 256\n",
        "CROP_SIZE        = 224\n",
        "MEAN             = [0.485, 0.456, 0.406]\n",
        "STD              = [0.229, 0.224, 0.225]\n",
        "\n",
        "BATCH            = 16\n",
        "VAL_SPLIT        = 0.10\n",
        "OUT_INDICES      = (2, 3)       # timm ResNet18: layer2, layer3\n",
        "TOP_P            = 0.02         # 画像スコア＝上位p%平均\n",
        "TAU_PERC         = 99.5         # val分布の分位点\n",
        "BLUR_SIGMA       = 2.0          # ヒートマップ平滑化 (0/None で無効)\n",
        "\n",
        "# PatchCore: コアセット\n",
        "MAX_CANDIDATES   = 80000        # 事前ランダム間引き（候補の上限）\n",
        "CORESET_SIZE     = 10000        # 代表ベクトル数（≒メモリバンク）\n",
        "KMEANS_BATCH     = 2048         # MiniBatchKMeans のバッチサイズ\n",
        "\n",
        "SAVE_DIR         = \"/content/out_patchcore_timm\"\n",
        "SEED             = 0\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "np.random.seed(SEED); random.seed(SEED); torch.manual_seed(SEED)\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# ---------------- Utility ----------------\n",
        "IMG_EXTS = (\".png\",\".jpg\",\".jpeg\",\".bmp\",\".tif\",\".tiff\")\n",
        "def list_images_multi_ext(dirp):\n",
        "    files=[]\n",
        "    for r,_,fs in os.walk(dirp):\n",
        "        for f in fs:\n",
        "            if os.path.splitext(f)[1].lower() in IMG_EXTS:\n",
        "                files.append(os.path.join(r,f))\n",
        "    return sorted(files)\n",
        "\n",
        "def autodetect_root(cls: str):\n",
        "    hits=[]\n",
        "    for base in ROOT_CANDIDATES:\n",
        "        p_tr = os.path.join(base, cls, \"train\", \"good\")\n",
        "        p_te = os.path.join(base, cls, \"test\")\n",
        "        if os.path.isdir(p_tr) and os.path.isdir(p_te):\n",
        "            hits.append(base)\n",
        "    if not hits:\n",
        "        for base in [\"/content\"]:\n",
        "            for p in glob.glob(os.path.join(base, \"**\", cls, \"train\", \"good\"), recursive=True):\n",
        "                root = p.split(f\"/{cls}/train/\")[0]\n",
        "                if os.path.isdir(os.path.join(root, cls, \"test\")):\n",
        "                    hits.append(root)\n",
        "    if not hits:\n",
        "        raise FileNotFoundError(f\"[ERROR] Dataset for class='{cls}' not found under /content.\")\n",
        "    hits = sorted(set(hits), key=lambda x: len(x))\n",
        "    return hits[0]\n",
        "\n",
        "def print_dataset_summary(root, cls):\n",
        "    tg  = os.path.join(root, cls, \"train\", \"good\")\n",
        "    ttg = os.path.join(root, cls, \"test\", \"good\")\n",
        "    tdir= os.path.join(root, cls, \"test\")\n",
        "    def _c(p): return len(list_images_multi_ext(p)) if os.path.isdir(p) else 0\n",
        "    subs=[]\n",
        "    if os.path.isdir(tdir):\n",
        "        subs=[d for d in os.listdir(tdir) if os.path.isdir(os.path.join(tdir,d)) and d not in [\"good\",\"ground_truth\"]]\n",
        "    n_an=0\n",
        "    for d in subs: n_an += _c(os.path.join(tdir,d))\n",
        "    print(\"[SUMMARY]\", {\"root\":root, \"class\":cls, \"train/good\":_c(tg),\n",
        "                        \"test/good\":_c(ttg), \"test/anoms\":n_an, \"anomaly_types\":subs})\n",
        "\n",
        "# ---------------- Datasets ----------------\n",
        "_tf = transforms.Compose([\n",
        "    transforms.Resize(IMG_SIZE, interpolation=Image.BILINEAR),\n",
        "    transforms.CenterCrop(CROP_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=MEAN, std=STD),\n",
        "])\n",
        "\n",
        "class MVTecTrain(Dataset):\n",
        "    def __init__(self, root, cls, val_split=0.1, is_val=False):\n",
        "        base = os.path.join(root, cls, \"train\", \"good\")\n",
        "        files = list_images_multi_ext(base)\n",
        "        assert len(files)>0, f\"No train images in {base}\"\n",
        "        n_val = max(1, int(len(files)*val_split))\n",
        "        self.files = files[:n_val] if is_val else files[n_val:]\n",
        "        if is_val and len(self.files)==0: self.files = files[-n_val:]\n",
        "    def __len__(self): return len(self.files)\n",
        "    def __getitem__(self, i):\n",
        "        img = Image.open(self.files[i]).convert(\"RGB\")\n",
        "        return _tf(img), 0\n",
        "\n",
        "class MVTecTest(Dataset):\n",
        "    def __init__(self, root, cls):\n",
        "        base = os.path.join(root, cls, \"test\")\n",
        "        goods = list_images_multi_ext(os.path.join(base, \"good\"))\n",
        "        anoms, types = [], []\n",
        "        for d in os.listdir(base):\n",
        "            p = os.path.join(base, d)\n",
        "            if os.path.isdir(p) and d not in [\"good\",\"ground_truth\"]:\n",
        "                fs = list_images_multi_ext(p)\n",
        "                anoms.extend(fs); types.extend([d]*len(fs))\n",
        "        self.files  = goods + anoms\n",
        "        self.labels = [0]*len(goods) + [1]*len(anoms)\n",
        "        self.types  = [\"good\"]*len(goods) + types\n",
        "        assert len(self.files)>0, f\"No test images in {base}\"\n",
        "    def __len__(self): return len(self.files)\n",
        "    def __getitem__(self, i):\n",
        "        img = Image.open(self.files[i]).convert(\"RGB\")\n",
        "        x = _tf(img)\n",
        "        return x, self.labels[i], self.files[i], self.types[i]\n",
        "\n",
        "# ---------------- Backbone (timm ResNet18) ----------------\n",
        "backbone = timm.create_model(\n",
        "    'resnet18.a1_in1k', pretrained=True, features_only=True, out_indices=OUT_INDICES\n",
        ").to(device).eval()\n",
        "\n",
        "def extract_feats(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    x: (B,3,H,W) → list[ (B,C_i,H_i,W_i) ]  (layer2, layer3)\n",
        "    layer3 を layer2 の空間解像度に upsample → チャネル連結 → L2正規化\n",
        "    返り: (B, C_total, Ht, Wt)\n",
        "    \"\"\"\n",
        "    feats = backbone(x)\n",
        "    f2, f3 = feats[0], feats[1]         # e.g., (B,128,28,28), (B,256,14,14)\n",
        "    f3u = F.interpolate(f3, size=f2.shape[2:], mode='bilinear', align_corners=False)\n",
        "    f = torch.cat([f2, f3u], dim=1)     # (B,384,28,28)\n",
        "    f = F.normalize(f, p=2, dim=1)\n",
        "    return f\n",
        "\n",
        "def fmap_to_desc(f: torch.Tensor) -> torch.Tensor:\n",
        "    # (B,C,H,W) → (B*P, C)\n",
        "    B,C,H,W = f.shape\n",
        "    return f.permute(0,2,3,1).contiguous().view(B*H*W, C)\n",
        "\n",
        "# ---------------- Dataset/Loader 準備 ----------------\n",
        "root = autodetect_root(CLASS_NAME)\n",
        "print_dataset_summary(root, CLASS_NAME)\n",
        "\n",
        "ds_tr = MVTecTrain(root, CLASS_NAME, val_split=VAL_SPLIT, is_val=False)\n",
        "ds_va = MVTecTrain(root, CLASS_NAME, val_split=VAL_SPLIT, is_val=True)\n",
        "ds_te = MVTecTest (root, CLASS_NAME)\n",
        "\n",
        "dl_tr = DataLoader(ds_tr, batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
        "dl_va = DataLoader(ds_va, batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
        "dl_te = DataLoader(ds_te, batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"[INFO] train={len(ds_tr)} | val={len(ds_va)} | test={len(ds_te)}\")\n",
        "\n",
        "# ---------------- PatchCore: メモリバンク（コアセット）構築 ----------------\n",
        "# 1) train/good のパッチ記述子を収集\n",
        "train_desc_list = []\n",
        "for xb, _ in dl_tr:\n",
        "    xb = xb.to(device)\n",
        "    fmap = extract_feats(xb)                # (B,Ct,Ht,Wt)\n",
        "    desc = fmap_to_desc(fmap).detach().cpu()# (B*P,Ct)\n",
        "    train_desc_list.append(desc)\n",
        "ALL = torch.cat(train_desc_list, dim=0).numpy().astype(np.float32)  # (N_all, Ct)\n",
        "N_all, Ct = ALL.shape\n",
        "print(f\"[PATCHES] collected: {N_all} x {Ct}\")\n",
        "\n",
        "# 2) 候補をランダム間引き → MiniBatchKMeansでコアセット（代表ベクトル）を作成\n",
        "if N_all > MAX_CANDIDATES:\n",
        "    idx = np.random.RandomState(SEED).permutation(N_all)[:MAX_CANDIDATES]\n",
        "    CANDS = ALL[idx]\n",
        "else:\n",
        "    CANDS = ALL\n",
        "n_clusters = int(min(len(CANDS), CORESET_SIZE))\n",
        "print(f\"[CORESET] candidates={len(CANDS)} → clusters={n_clusters}\")\n",
        "\n",
        "kmeans = MiniBatchKMeans(\n",
        "    n_clusters=n_clusters, batch_size=KMEANS_BATCH,\n",
        "    init=\"k-means++\", max_no_improvement=20, n_init=\"auto\", random_state=SEED,\n",
        "    reassignment_ratio=0.01, verbose=0\n",
        ")\n",
        "kmeans.fit(CANDS)\n",
        "MEM = kmeans.cluster_centers_.astype(np.float32)  # (n_clusters, Ct)\n",
        "print(f\"[MEM] centers: {MEM.shape}\")\n",
        "\n",
        "# 3) 1-NN 検索器\n",
        "nn_index = NearestNeighbors(n_neighbors=1, algorithm=\"brute\", metric=\"euclidean\")\n",
        "nn_index.fit(MEM)\n",
        "\n",
        "# ---------------- スコア計算（位置マップ & 画像スコア） ----------------\n",
        "def batch_patchcore_scores(xb: torch.Tensor) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    xb: (B,3,H,W)\n",
        "    返り:\n",
        "      amap_up: (B, CROP_SIZE, CROP_SIZE) 位置スコア（1-NN距離）\n",
        "      img_sc : (B,) 上位TOP_P%平均の画像スコア\n",
        "    \"\"\"\n",
        "    fmap = extract_feats(xb)                    # (B,Ct,Ht,Wt)\n",
        "    B, Ct, Ht, Wt = fmap.shape\n",
        "    desc = fmap_to_desc(fmap).cpu().numpy().astype(np.float32)  # (B*P, Ct)\n",
        "    dists, _ = nn_index.kneighbors(desc, n_neighbors=1, return_distance=True)  # (B*P,1)\n",
        "    amap = dists.reshape(B, Ht, Wt)\n",
        "\n",
        "    # 平滑化 → 画像サイズに拡大\n",
        "    if BLUR_SIGMA and BLUR_SIGMA>0:\n",
        "        amap = np.stack([gaussian_filter(a, sigma=BLUR_SIGMA) for a in amap], axis=0)\n",
        "    amap_t = torch.from_numpy(amap).unsqueeze(1).float()\n",
        "    amap_up = F.interpolate(amap_t, size=(CROP_SIZE, CROP_SIZE), mode='bilinear', align_corners=False).squeeze(1).numpy()\n",
        "\n",
        "    # 画像スコア（上位p%平均）\n",
        "    flat = amap_up.reshape(B, -1)\n",
        "    k = max(1, int(flat.shape[1] * TOP_P))\n",
        "    img_sc = np.array([np.mean(np.partition(flat[i], -k)[-k:]) for i in range(B)], dtype=np.float32)\n",
        "    return amap_up, img_sc\n",
        "\n",
        "# ---------------- 閾値（val） ----------------\n",
        "val_scores = []\n",
        "for xb, _ in dl_va:\n",
        "    xb = xb.to(device)\n",
        "    amap_up, sc = batch_patchcore_scores(xb)\n",
        "    val_scores.extend(sc.tolist())\n",
        "val_scores = np.asarray(val_scores, dtype=np.float32)\n",
        "tau = float(np.percentile(val_scores, TAU_PERC))\n",
        "print(f\"[TAU] from val {TAU_PERC}% → tau={tau:.6f}\")\n",
        "\n",
        "# ---------------- Test 推論＆評価 ----------------\n",
        "all_scores, all_labels, all_amaps, all_paths, all_types = [], [], [], [], []\n",
        "for xb, yb, paths, types in dl_te:\n",
        "    xb = xb.to(device)\n",
        "    amap_up, sc = batch_patchcore_scores(xb)\n",
        "    all_scores.extend(sc.tolist())\n",
        "    all_labels.extend(yb.numpy().tolist())\n",
        "    all_paths.extend(list(paths))\n",
        "    all_types.extend(list(types))\n",
        "    all_amaps.extend([a for a in amap_up])\n",
        "\n",
        "scores_np = np.asarray(all_scores, dtype=np.float32)\n",
        "labels_np = np.asarray(all_labels, dtype=np.int32)\n",
        "\n",
        "auroc_img = roc_auc_score(labels_np, scores_np)\n",
        "prec, rec, _ = precision_recall_curve(labels_np, scores_np)\n",
        "ap_img = average_precision_score(labels_np, scores_np)\n",
        "print(f\"[IMAGE] AUROC={auroc_img:.4f} | AP={ap_img:.4f}\")\n",
        "\n",
        "# 曲線表示\n",
        "fpr, tpr, thr = roc_curve(labels_np, scores_np)\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1); plt.plot(fpr,tpr,label=f\"AUC={auroc_img:.4f}\"); plt.plot([0,1],[0,1],'--',alpha=0.5)\n",
        "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC (PatchCore, timm-resnet18)\"); plt.legend()\n",
        "plt.subplot(1,2,2); plt.plot(rec,prec,label=f\"AP={ap_img:.4f}\")\n",
        "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"PR (PatchCore)\"); plt.legend()\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# ---------------- 可視化 ----------------\n",
        "def imread_rgb(p):\n",
        "    img = Image.open(p).convert(\"RGB\")\n",
        "    return np.array(img)\n",
        "\n",
        "def overlay_show(img_rgb, amap, title, alpha=0.5):\n",
        "    a = amap\n",
        "    if a.max() > 0:\n",
        "        a = (a - a.min()) / (a.max() - a.min() + 1e-8)\n",
        "    plt.figure(figsize=(8,3))\n",
        "    ax1 = plt.subplot(1,2,1); ax1.imshow(img_rgb); ax1.set_title(\"image\"); ax1.axis(\"off\")\n",
        "    ax2 = plt.subplot(1,2,2); ax2.imshow(img_rgb); ax2.imshow(a, cmap=\"jet\", alpha=alpha); ax2.set_title(title); ax2.axis(\"off\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# Top-K 異常\n",
        "order = np.argsort(-scores_np)\n",
        "TOPK  = min(6, len(order))\n",
        "print(f\"[SHOW] Top-{TOPK} anomaly candidates\")\n",
        "for r in range(TOPK):\n",
        "    i = int(order[r])\n",
        "    img = imread_rgb(all_paths[i])\n",
        "    overlay_show(img, all_amaps[i], f\"score={scores_np[i]:.6f} | pred={int(scores_np[i]>tau)} | true={labels_np[i]}\")\n",
        "\n",
        "# しきい値で混同行列風の枚数\n",
        "preds = (scores_np > tau).astype(np.int32)\n",
        "TP = np.where((preds==1)&(labels_np==1))[0]\n",
        "FP = np.where((preds==1)&(labels_np==0))[0]\n",
        "FN = np.where((preds==0)&(labels_np==1))[0]\n",
        "print(f\"[CONF] TP={len(TP)} FP={len(FP)} FN={len(FN)} TN={len(labels_np)-len(TP)-len(FN)-len(FP)}\")\n",
        "\n",
        "# 誤検出/見逃し 例\n",
        "def show_examples(idxs, title):\n",
        "    if len(idxs)==0:\n",
        "        print(f\"[SHOW] {title}: none\"); return\n",
        "    print(f\"[SHOW] {title} examples\")\n",
        "    for i in idxs[:3]:\n",
        "        img = imread_rgb(all_paths[i])\n",
        "        overlay_show(img, all_amaps[i], f\"{title} | score={scores_np[i]:.6f}\")\n",
        "\n",
        "show_examples(FP, \"False Positive\")\n",
        "show_examples(FN, \"False Negative\")\n",
        "\n",
        "# ---------------- 保存 ----------------\n",
        "np.save(os.path.join(SAVE_DIR, f\"scores_{CLASS_NAME}.npy\"), scores_np)\n",
        "np.save(os.path.join(SAVE_DIR, f\"labels_{CLASS_NAME}.npy\"), labels_np)\n",
        "print(f\"[SAVED] {SAVE_DIR}/scores_{CLASS_NAME}.npy, labels_{CLASS_NAME}.npy\")\n"
      ],
      "metadata": {
        "id": "mBayp11sD69h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vXlKM8ulFE-e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}