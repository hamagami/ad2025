{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyN/AK+gIA+IbrQc6dvf1w9V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamagami/ad2025/blob/main/PaDim_timm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PaDim: Patch Distribution Modeling"
      ],
      "metadata": {
        "id": "SJIHo46wTpvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# セル1: MVTec AD を mydrive 直リンクからダウンロード＆展開（Drive不要）\n",
        "# - TARGET=\"all\" なら全体 (~5GB)\n",
        "# - それ以外はクラス名（例: \"bottle\"）のみをダウンロード\n",
        "# - 展開先: /content/mvtec_anomaly_detection/<class>/...\n",
        "# - 互換リンク: /content/mvtec -> /content/mvtec_anomaly_detection\n",
        "# =====================================================\n",
        "\n",
        "import os, subprocess, tarfile, zipfile, glob\n",
        "\n",
        "TARGET = \"bottle\"  # \"all\" または 15クラス名のいずれか: bottle,cable,capsule,carpet,grid,hazelnut,leather,metal_nut,pill,screw,tile,toothbrush,transistor,wood,zipper\n",
        "DEST_PARENT = \"/content\"\n",
        "DEST_ROOT   = os.path.join(DEST_PARENT, \"mvtec_anomaly_detection\")\n",
        "COMP_LINK   = \"/content/mvtec\"  # 互換リンク\n",
        "\n",
        "WHOLE_URL = \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938113-1629952094/mvtec_anomaly_detection.tar.xz\"\n",
        "CLASS_URLS = {\n",
        "    \"bottle\":      \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937370-1629951468/bottle.tar.xz\",\n",
        "    \"cable\":       \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937413-1629951498/cable.tar.xz\",\n",
        "    \"capsule\":     \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937454-1629951595/capsule.tar.xz\",\n",
        "    \"carpet\":      \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937484-1629951672/carpet.tar.xz\",\n",
        "    \"grid\":        \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937487-1629951814/grid.tar.xz\",\n",
        "    \"hazelnut\":    \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937545-1629951845/hazelnut.tar.xz\",\n",
        "    \"leather\":     \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937607-1629951964/leather.tar.xz\",\n",
        "    \"metal_nut\":   \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937637-1629952063/metal_nut.tar.xz\",\n",
        "    \"pill\":        \"https://www.mydrive.ch/shares/43421/11a215a5749fcfb75e331ddd5f8e43ee/download/420938129-1629953099/pill.tar.xz\",\n",
        "    \"screw\":       \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938130-1629953152/screw.tar.xz\",\n",
        "    \"tile\":        \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938133-1629953189/tile.tar.xz\",\n",
        "    \"toothbrush\":  \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938134-1629953256/toothbrush.tar.xz\",\n",
        "    \"transistor\":  \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938166-1629953277/transistor.tar.xz\",\n",
        "    \"wood\":        \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938383-1629953354/wood.tar.xz\",\n",
        "    \"zipper\":      \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938385-1629953449/zipper.tar.xz\",\n",
        "}\n",
        "IMG_EXTS = ('*.png','*.jpg','*.jpeg','*.bmp','*.tif','*.tiff')\n",
        "\n",
        "def run(cmd):\n",
        "    print(\"[RUN]\", cmd)\n",
        "    rc = subprocess.call(cmd, shell=True)\n",
        "    if rc != 0:\n",
        "        raise RuntimeError(f\"Command failed (rc={rc}): {cmd}\")\n",
        "\n",
        "def list_images_multi_ext(dirpath: str):\n",
        "    files=[]\n",
        "    for ext in IMG_EXTS:\n",
        "        files += glob.glob(os.path.join(dirpath, '**', ext), recursive=True)\n",
        "    return sorted(files)\n",
        "\n",
        "os.makedirs(DEST_ROOT, exist_ok=True)\n",
        "\n",
        "if TARGET == \"all\":\n",
        "    tar_path = os.path.join(DEST_PARENT, \"mvtec_ad.tar.xz\")\n",
        "    run(f'wget --progress=dot:mega -O \"{tar_path}\" \"{WHOLE_URL}\"')\n",
        "    run(f'tar -xf \"{tar_path}\" -C \"{DEST_PARENT}\"')\n",
        "else:\n",
        "    if TARGET not in CLASS_URLS:\n",
        "        raise ValueError(f\"Unknown TARGET='{TARGET}'.\")\n",
        "    tar_path = os.path.join(DEST_PARENT, f\"{TARGET}.tar.xz\")\n",
        "    run(f'wget --progress=dot:mega -O \"{tar_path}\" \"{CLASS_URLS[TARGET]}\"')\n",
        "    run(f'tar -xf \"{tar_path}\" -C \"{DEST_ROOT}\"')\n",
        "\n",
        "# 互換リンク（/content/mvtec）が無ければ作成\n",
        "if not os.path.exists(COMP_LINK):\n",
        "    try:\n",
        "        os.symlink(DEST_ROOT, COMP_LINK)\n",
        "        print(\"[INFO] Symlink:\", COMP_LINK, \"->\", DEST_ROOT)\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] symlink failed:\", e)\n",
        "\n",
        "# 簡易チェック＆要約\n",
        "def quick_summary(root_parent=\"/content\"):\n",
        "    candidates = [\"/content/mvtec\", \"/content/mvtec_anomaly_detection\"]\n",
        "    ok = False\n",
        "    for base in candidates:\n",
        "        if not os.path.isdir(base): continue\n",
        "        classes = [d for d in os.listdir(base) if os.path.isdir(os.path.join(base, d))]\n",
        "        classes = sorted([c for c in classes if os.path.isdir(os.path.join(base,c,\"test\")) or os.path.isdir(os.path.join(base,c,\"train\"))])\n",
        "        if not classes: continue\n",
        "        print(f\"[READY] Found classes under {base}: {classes[:10]}{' ...' if len(classes)>10 else ''}\")\n",
        "        # 1クラスだけ簡易枚数\n",
        "        cls0 = classes[0]\n",
        "        tg = os.path.join(base, cls0, \"train\", \"good\")\n",
        "        ttg= os.path.join(base, cls0, \"test\",  \"good\")\n",
        "        n_tg = len(list_images_multi_ext(tg)) if os.path.isdir(tg) else 0\n",
        "        n_ttg= len(list_images_multi_ext(ttg)) if os.path.isdir(ttg) else 0\n",
        "        print(f\"[SUMMARY] sample class={cls0}: train/good={n_tg}, test/good={n_ttg}\")\n",
        "        ok = True\n",
        "    if not ok:\n",
        "        raise FileNotFoundError(\"Extraction finished but dataset layout not detected. Please check archive.\")\n",
        "    print(\"[DONE] MVTec AD is ready under /content/mvtec_anomaly_detection\")\n",
        "\n",
        "quick_summary()\n"
      ],
      "metadata": {
        "id": "Dn0zg-RdZW3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# セル2: PaDiM (timm/ResNet18, layer2+layer3, D=100)\n",
        "# - /content/mvtec_anomaly_detection/<class>/... を自動検出\n",
        "# - 学習: 位置ごとガウス(μ, Σ) 推定（チャネルをランダム100次元に縮約）\n",
        "# - 推論: Mahalanobis距離のマップ → 上位p%平均で画像スコア\n",
        "# - 閾値: train/good を分割した val の分位点\n",
        "# - 可視化: ROC/PR, Top異常, 誤検出/見逃し\n",
        "# ============================================================\n",
        "\n",
        "import os, glob, math, random\n",
        "from typing import List, Tuple, Dict\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# timm が無ければ入れる\n",
        "try:\n",
        "    import timm\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"timm\"], check=False)\n",
        "    import timm\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, average_precision_score\n",
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "# ---------------- Config ----------------\n",
        "CLASS_NAME       = globals().get(\"TARGET\", \"bottle\")  # セル1のTARGETに合わせる\n",
        "ROOT_CANDIDATES  = [\"/content/mvtec\", \"/content/mvtec_anomaly_detection\"]\n",
        "\n",
        "IMG_SIZE         = 256\n",
        "CROP_SIZE        = 224\n",
        "MEAN             = [0.485, 0.456, 0.406]\n",
        "STD              = [0.229, 0.224, 0.225]\n",
        "\n",
        "BATCH            = 16\n",
        "VAL_SPLIT        = 0.10\n",
        "OUT_INDICES      = (2, 3)        # timm ResNet18: layer2, layer3\n",
        "REDUCED_D        = 100           # PaDiM 次元\n",
        "TOP_P            = 0.02          # 画像スコア＝上位p%平均\n",
        "TAU_PERC         = 99.5          # val分布の分位点\n",
        "BLUR_SIGMA       = 2.0           # ヒートマップ平滑化(0/Noneで無効)\n",
        "EPS_COV          = 1e-5          # 共分散のダイアゴナル微小正則化\n",
        "SAVE_DIR         = \"/content/out_padim_timm\"\n",
        "\n",
        "SEED             = 0\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "np.random.seed(SEED); random.seed(SEED); torch.manual_seed(SEED)\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# ---------------- Utility ----------------\n",
        "IMG_EXTS = (\".png\",\".jpg\",\".jpeg\",\".bmp\",\".tif\",\".tiff\")\n",
        "def list_images_multi_ext(dirp):\n",
        "    files=[]\n",
        "    for r,_,fs in os.walk(dirp):\n",
        "        for f in fs:\n",
        "            if os.path.splitext(f)[1].lower() in IMG_EXTS:\n",
        "                files.append(os.path.join(r,f))\n",
        "    return sorted(files)\n",
        "\n",
        "def autodetect_root(cls: str):\n",
        "    hits=[]\n",
        "    for base in ROOT_CANDIDATES:\n",
        "        p_tr = os.path.join(base, cls, \"train\", \"good\")\n",
        "        p_te = os.path.join(base, cls, \"test\")\n",
        "        if os.path.isdir(p_tr) and os.path.isdir(p_te):\n",
        "            hits.append(base)\n",
        "    if not hits:\n",
        "        for base in [\"/content\"]:\n",
        "            for p in glob.glob(os.path.join(base, \"**\", cls, \"train\", \"good\"), recursive=True):\n",
        "                root = p.split(f\"/{cls}/train/\")[0]\n",
        "                if os.path.isdir(os.path.join(root, cls, \"test\")):\n",
        "                    hits.append(root)\n",
        "    if not hits:\n",
        "        raise FileNotFoundError(f\"[ERROR] Dataset for class='{cls}' not found under /content.\")\n",
        "    hits = sorted(set(hits), key=lambda x: len(x))\n",
        "    return hits[0]\n",
        "\n",
        "def print_dataset_summary(root, cls):\n",
        "    tg  = os.path.join(root, cls, \"train\", \"good\")\n",
        "    ttg = os.path.join(root, cls, \"test\", \"good\")\n",
        "    tdir= os.path.join(root, cls, \"test\")\n",
        "    def _c(p): return len(list_images_multi_ext(p)) if os.path.isdir(p) else 0\n",
        "    subs=[]\n",
        "    if os.path.isdir(tdir):\n",
        "        subs=[d for d in os.listdir(tdir) if os.path.isdir(os.path.join(tdir,d)) and d not in [\"good\",\"ground_truth\"]]\n",
        "    n_an=0\n",
        "    for d in subs: n_an += _c(os.path.join(tdir,d))\n",
        "    print(\"[SUMMARY]\", {\"root\":root, \"class\":cls, \"train/good\":_c(tg),\n",
        "                        \"test/good\":_c(ttg), \"test/anoms\":n_an, \"anomaly_types\":subs})\n",
        "\n",
        "# ---------------- Datasets ----------------\n",
        "_tf = transforms.Compose([\n",
        "    transforms.Resize(IMG_SIZE, interpolation=Image.BILINEAR),\n",
        "    transforms.CenterCrop(CROP_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=MEAN, std=STD),\n",
        "])\n",
        "\n",
        "class MVTecTrain(Dataset):\n",
        "    def __init__(self, root, cls, val_split=0.1, is_val=False):\n",
        "        base = os.path.join(root, cls, \"train\", \"good\")\n",
        "        files = list_images_multi_ext(base)\n",
        "        assert len(files)>0, f\"No train images in {base}\"\n",
        "        n_val = max(1, int(len(files)*val_split))\n",
        "        self.files = files[:n_val] if is_val else files[n_val:]\n",
        "        if is_val and len(self.files)==0: self.files = files[-n_val:]\n",
        "    def __len__(self): return len(self.files)\n",
        "    def __getitem__(self, i):\n",
        "        img = Image.open(self.files[i]).convert(\"RGB\")\n",
        "        return _tf(img), 0\n",
        "\n",
        "class MVTecTest(Dataset):\n",
        "    def __init__(self, root, cls):\n",
        "        base = os.path.join(root, cls, \"test\")\n",
        "        goods = list_images_multi_ext(os.path.join(base, \"good\"))\n",
        "        anoms, types = [], []\n",
        "        for d in os.listdir(base):\n",
        "            p = os.path.join(base, d)\n",
        "            if os.path.isdir(p) and d not in [\"good\",\"ground_truth\"]:\n",
        "                fs = list_images_multi_ext(p)\n",
        "                anoms.extend(fs); types.extend([d]*len(fs))\n",
        "        self.files  = goods + anoms\n",
        "        self.labels = [0]*len(goods) + [1]*len(anoms)\n",
        "        self.types  = [\"good\"]*len(goods) + types\n",
        "        assert len(self.files)>0, f\"No test images in {base}\"\n",
        "    def __len__(self): return len(self.files)\n",
        "    def __getitem__(self, i):\n",
        "        img = Image.open(self.files[i]).convert(\"RGB\")\n",
        "        x = _tf(img)\n",
        "        return x, self.labels[i], self.files[i], self.types[i]"
      ],
      "metadata": {
        "id": "2aLtqM7MT9TS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Backbone (timm ResNet18) ----------------\n",
        "backbone = timm.create_model(\n",
        "    'resnet18.a1_in1k', pretrained=True, features_only=True, out_indices=OUT_INDICES\n",
        ").to(device).eval()\n",
        "\n",
        "def extract_feats(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    x: (B,3,H,W) → list[ (B,C_i,H_i,W_i) ]  (layer2, layer3)\n",
        "    layer3 を layer2 の空間解像度に upsample → チャネル連結 → L2正規化\n",
        "    返り: (B, C_total, Ht, Wt)  ※ここをベース解像度として PaDiM を構築\n",
        "    \"\"\"\n",
        "    feats = backbone(x)\n",
        "    f2, f3 = feats[0], feats[1]         # e.g., (B,128,28,28), (B,256,14,14)\n",
        "    f3u = F.interpolate(f3, size=f2.shape[2:], mode='bilinear', align_corners=False)\n",
        "    f = torch.cat([f2, f3u], dim=1)     # (B,384,28,28)\n",
        "    f = F.normalize(f, p=2, dim=1)\n",
        "    return f\n",
        "\n",
        "def fmap_to_desc(f: torch.Tensor) -> torch.Tensor:\n",
        "    # (B,C,H,W) → (B, P=H*W, C)\n",
        "    B,C,H,W = f.shape\n",
        "    return f.permute(0,2,3,1).contiguous().view(B, H*W, C)"
      ],
      "metadata": {
        "id": "VReDpoRFUL-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Dataset/Loader 準備 ----------------\n",
        "root = autodetect_root(CLASS_NAME)\n",
        "print_dataset_summary(root, CLASS_NAME)\n",
        "\n",
        "ds_tr = MVTecTrain(root, CLASS_NAME, val_split=VAL_SPLIT, is_val=False)\n",
        "ds_va = MVTecTrain(root, CLASS_NAME, val_split=VAL_SPLIT, is_val=True)\n",
        "ds_te = MVTecTest (root, CLASS_NAME)\n",
        "\n",
        "dl_tr = DataLoader(ds_tr, batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
        "dl_va = DataLoader(ds_va, batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
        "dl_te = DataLoader(ds_te, batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"[INFO] train={len(ds_tr)} | val={len(ds_va)} | test={len(ds_te)}\")\n",
        "\n",
        "# ---------------- PaDiM: μ, Σ（→Cholesky） 学習 ----------------\n",
        "# まず1バッチで形を確定し、チャネル縮約(ランダム100次元)のインデックスを決定\n",
        "xb0, _ = next(iter(dl_tr))\n",
        "xb0 = xb0.to(device)\n",
        "f0 = extract_feats(xb0)                   # (B,Ct,Ht,Wt)\n",
        "_, Ct, Ht, Wt = f0.shape\n",
        "P = Ht*Wt\n",
        "rng = torch.Generator().manual_seed(SEED)\n",
        "CH_IDX = torch.randperm(Ct, generator=rng)[:REDUCED_D].to(device)   # (D,)\n",
        "print(f\"[PaDiM] base fmap: C={Ct}, HxW={Ht}x{Wt}, reduced D={REDUCED_D}\")\n",
        "\n",
        "# 累積量（float64で精度安定）：sum_x (P,D), sum_xxT (P,D,D), n枚数\n",
        "sum_x   = torch.zeros(P, REDUCED_D, dtype=torch.float64)\n",
        "sum_xxT = torch.zeros(P, REDUCED_D, REDUCED_D, dtype=torch.float64)\n",
        "n_imgs  = 0\n",
        "\n",
        "for xb, _ in dl_tr:\n",
        "    xb = xb.to(device)\n",
        "    f  = extract_feats(xb)                # (B,Ct,Ht,Wt)\n",
        "    f  = f[:, CH_IDX, :, :]               # (B,D,Ht,Wt)\n",
        "    desc = fmap_to_desc(f).cpu().double() # (B,P,D)\n",
        "    B = desc.size(0)\n",
        "    # sum_x\n",
        "    sum_x += desc.sum(dim=0)              # (P,D)\n",
        "    # sum_xxT : 各画像で (P,D,D) の outer を積んでバッチ加算\n",
        "    #  einsum('bpd,bpe->pde') でもOK\n",
        "    for b in range(B):\n",
        "        xb_ = desc[b]                     # (P,D)\n",
        "        sum_xxT += torch.einsum('pd,pe->pde', xb_, xb_)\n",
        "    n_imgs += B\n",
        "\n",
        "mu = (sum_x / max(1, n_imgs)).float()     # (P,D)\n",
        "Exx = (sum_xxT / max(1, n_imgs)).float()  # (P,D,D)\n",
        "# 共分散: E[xx^T] - μμ^T\n",
        "mu_outer = torch.einsum('pd,pe->pde', mu, mu)  # (P,D,D)\n",
        "cov = Exx - mu_outer                           # (P,D,D)\n",
        "\n",
        "# 共分散 正則化 + Cholesky（失敗時はジッターを増やす）\n",
        "I = torch.eye(REDUCED_D).unsqueeze(0).repeat(P,1,1)\n",
        "cov = cov + EPS_COV * I\n",
        "L = None\n",
        "for jitter in [0.0, 1e-6, 1e-5, 1e-4, 1e-3]:\n",
        "    try:\n",
        "        L = torch.linalg.cholesky(cov + jitter*I)\n",
        "        if jitter>0: print(f\"[INFO] cholesky jitter used: {jitter}\")\n",
        "        break\n",
        "    except RuntimeError:\n",
        "        if jitter == 1e-3:\n",
        "            raise\n",
        "        continue\n",
        "\n",
        "# 推論用に GPU へ\n",
        "mu_d = mu.to(device)            # (P,D)\n",
        "L_d  = L.to(device)             # (P,D,D)\n",
        "\n",
        "# ---------------- スコア計算関数（Mahalanobis） ----------------\n",
        "@torch.no_grad()\n",
        "def batch_padim_scores(xb: torch.Tensor) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    xb: (B,3,H,W)\n",
        "    返り:\n",
        "      amap_up: (B, CROP_SIZE, CROP_SIZE) 位置スコア（Mahalanobis^2）\n",
        "      img_sc : (B,) 上位TOP_P%平均の画像スコア\n",
        "    \"\"\"\n",
        "    f = extract_feats(xb)                # (B,Ct,Ht,Wt)\n",
        "    f = f[:, CH_IDX, :, :]              # (B,D,Ht,Wt)\n",
        "    desc = fmap_to_desc(f)              # (B,P,D)\n",
        "    B = desc.size(0)\n",
        "    # delta: (B,P,D)\n",
        "    delta = (desc - mu_d.unsqueeze(0))  # broadcast\n",
        "    # solve L y = delta^T  → y: (P,D,B)\n",
        "    Bmat = delta.permute(1,2,0)         # (P,D,B)\n",
        "    y = torch.linalg.solve_triangular(L_d, Bmat, upper=False)  # (P,D,B)\n",
        "    d2 = (y**2).sum(dim=1).permute(1,0) # (B,P)\n",
        "    amap = d2.reshape(B, Ht, Wt).detach().cpu().numpy()\n",
        "\n",
        "    # （任意）平滑化 → 画像サイズへ拡大\n",
        "    if BLUR_SIGMA and BLUR_SIGMA>0:\n",
        "        amap = np.stack([gaussian_filter(a, sigma=BLUR_SIGMA) for a in amap], axis=0)\n",
        "    amap_t = torch.from_numpy(amap).unsqueeze(1).float()\n",
        "    amap_up = F.interpolate(amap_t, size=(CROP_SIZE, CROP_SIZE), mode='bilinear', align_corners=False).squeeze(1).numpy()\n",
        "\n",
        "    # 画像スコア（上位p%平均）\n",
        "    flat = amap_up.reshape(B, -1)\n",
        "    k = max(1, int(flat.shape[1] * TOP_P))\n",
        "    img_sc = np.array([np.mean(np.partition(flat[i], -k)[-k:]) for i in range(B)], dtype=np.float32)\n",
        "    return amap_up, img_sc\n",
        "\n",
        "# ---------------- 閾値（val） ----------------\n",
        "val_scores = []\n",
        "for xb, _ in dl_va:\n",
        "    xb = xb.to(device)\n",
        "    amap_up, sc = batch_padim_scores(xb)\n",
        "    val_scores.extend(sc.tolist())\n",
        "val_scores = np.asarray(val_scores, dtype=np.float32)\n",
        "tau = float(np.percentile(val_scores, TAU_PERC))\n",
        "print(f\"[TAU] from val {TAU_PERC}% → tau={tau:.6f}\")\n"
      ],
      "metadata": {
        "id": "v12ymyG2U5I9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Test 推論＆評価 ----------------\n",
        "all_scores, all_labels, all_amaps, all_paths, all_types = [], [], [], [], []\n",
        "for xb, yb, paths, types in dl_te:\n",
        "    xb = xb.to(device)\n",
        "    amap_up, sc = batch_padim_scores(xb)\n",
        "    all_scores.extend(sc.tolist())\n",
        "    all_labels.extend(yb.numpy().tolist())\n",
        "    all_paths.extend(list(paths))\n",
        "    all_types.extend(list(types))\n",
        "    all_amaps.extend([a for a in amap_up])\n",
        "\n",
        "scores_np = np.asarray(all_scores, dtype=np.float32)\n",
        "labels_np = np.asarray(all_labels, dtype=np.int32)\n",
        "\n",
        "auroc_img = roc_auc_score(labels_np, scores_np)\n",
        "prec, rec, _ = precision_recall_curve(labels_np, scores_np)\n",
        "ap_img = average_precision_score(labels_np, scores_np)\n",
        "print(f\"[IMAGE] AUROC={auroc_img:.4f} | AP={ap_img:.4f}\")\n",
        "\n",
        "# 曲線表示\n",
        "fpr, tpr, thr = roc_curve(labels_np, scores_np)\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1); plt.plot(fpr,tpr,label=f\"AUC={auroc_img:.4f}\"); plt.plot([0,1],[0,1],'--',alpha=0.5)\n",
        "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC (PaDiM, timm-resnet18)\"); plt.legend()\n",
        "plt.subplot(1,2,2); plt.plot(rec,prec,label=f\"AP={ap_img:.4f}\")\n",
        "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"PR (PaDiM)\"); plt.legend()\n",
        "plt.tight_layout(); plt.show()"
      ],
      "metadata": {
        "id": "v-BPbF45VE_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- 可視化 ----------------\n",
        "def imread_rgb(p):\n",
        "    img = Image.open(p).convert(\"RGB\")\n",
        "    return np.array(img)\n",
        "\n",
        "def overlay_show(img_rgb, amap, title, alpha=0.5):\n",
        "    a = amap\n",
        "    if a.max() > 0:\n",
        "        a = (a - a.min()) / (a.max() - a.min() + 1e-8)\n",
        "    plt.figure(figsize=(8,3))\n",
        "    ax1 = plt.subplot(1,2,1); ax1.imshow(img_rgb); ax1.set_title(\"image\"); ax1.axis(\"off\")\n",
        "    ax2 = plt.subplot(1,2,2); ax2.imshow(img_rgb); ax2.imshow(a, cmap=\"jet\", alpha=alpha); ax2.set_title(title); ax2.axis(\"off\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# Top-K 異常\n",
        "order = np.argsort(-scores_np)\n",
        "TOPK  = min(6, len(order))\n",
        "print(f\"[SHOW] Top-{TOPK} anomaly candidates\")\n",
        "for r in range(TOPK):\n",
        "    i = int(order[r])\n",
        "    img = imread_rgb(all_paths[i])\n",
        "    overlay_show(img, all_amaps[i], f\"score={scores_np[i]:.6f} | pred={int(scores_np[i]>tau)} | true={labels_np[i]}\")\n",
        "\n",
        "# しきい値で混同行列風の枚数\n",
        "preds = (scores_np > tau).astype(np.int32)\n",
        "TP = np.where((preds==1)&(labels_np==1))[0]\n",
        "FP = np.where((preds==1)&(labels_np==0))[0]\n",
        "FN = np.where((preds==0)&(labels_np==1))[0]\n",
        "print(f\"[CONF] TP={len(TP)} FP={len(FP)} FN={len(FN)} TN={len(labels_np)-len(TP)-len(FP)-len(FN)}\")\n",
        "\n",
        "# 誤検出/見逃し 例\n",
        "def show_examples(idxs, title):\n",
        "    if len(idxs)==0:\n",
        "        print(f\"[SHOW] {title}: none\"); return\n",
        "    print(f\"[SHOW] {title} examples\")\n",
        "    for i in idxs[:3]:\n",
        "        img = imread_rgb(all_paths[i])\n",
        "        overlay_show(img, all_amaps[i], f\"{title} | score={scores_np[i]:.6f}\")\n",
        "\n",
        "show_examples(FP, \"False Positive\")\n",
        "show_examples(FN, \"False Negative\")\n",
        "\n",
        "# ---------------- 保存 ----------------\n",
        "np.save(os.path.join(SAVE_DIR, f\"scores_{CLASS_NAME}.npy\"), scores_np)\n",
        "np.save(os.path.join(SAVE_DIR, f\"labels_{CLASS_NAME}.npy\"), labels_np)\n",
        "print(f\"[SAVED] {SAVE_DIR}/scores_{CLASS_NAME}.npy, labels_{CLASS_NAME}.npy\")\n"
      ],
      "metadata": {
        "id": "Yc5RMRGsiUK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Reset notebook metadata in Colab ====\n",
        "# 使い方:\n",
        "#   MODE = \"soft\"  → widgetsメタだけ除去（最小改変）\n",
        "#   MODE = \"hard\"  → kernelspec/language_info 以外のメタを全消し（確実）\n",
        "#   STRIP_OUTPUTS = True で出力と実行番号も削除（軽量化＆差分きれい）\n",
        "\n",
        "MODE = \"hard\"          # \"soft\" or \"hard\"\n",
        "STRIP_OUTPUTS = False  # True で全出力/実行番号を削除\n",
        "\n",
        "from google.colab import _message\n",
        "import nbformat as nbf\n",
        "from pathlib import Path\n",
        "\n",
        "nb_dict = _message.blocking_request('get_ipynb')['ipynb']\n",
        "name = nb_dict.get('metadata',{}).get('colab',{}).get('name','notebook.ipynb')\n",
        "\n",
        "def reset_metadata(nb, mode=\"hard\", strip_outputs=False):\n",
        "    md = nb.get('metadata', {})\n",
        "    if mode == \"soft\":\n",
        "        # widgetsだけ消す\n",
        "        md.pop('widgets', None)\n",
        "    else:  # hard\n",
        "        # 必要最低限だけ残す\n",
        "        ks = md.get('kernelspec', {'display_name':'Python 3','language':'python','name':'python3'})\n",
        "        li = md.get('language_info', {'name':'python','pygments_lexer':'ipython3'})\n",
        "        nb['metadata'] = {'kernelspec': ks, 'language_info': li}\n",
        "\n",
        "    for c in nb.get('cells', []):\n",
        "        # セルのwidgetsメタを除去（soft） or 全メタ初期化（hard）\n",
        "        if mode == \"soft\":\n",
        "            c.get('metadata', {}).pop('widgets', None)\n",
        "        else:\n",
        "            c['metadata'] = {}\n",
        "\n",
        "        # ウィジェット出力を除去\n",
        "        if c.get('cell_type') == 'code' and 'outputs' in c:\n",
        "            new_out = []\n",
        "            for out in c['outputs']:\n",
        "                data = out.get('data', None)\n",
        "                if isinstance(data, dict):\n",
        "                    data.pop('application/vnd.jupyter.widget-view+json', None)\n",
        "                    data.pop('application/vnd.jupyter.widget-state+json', None)\n",
        "                    if not data:\n",
        "                        continue\n",
        "                if not strip_outputs:\n",
        "                    new_out.append(out)\n",
        "            c['outputs'] = [] if strip_outputs else new_out\n",
        "            if strip_outputs:\n",
        "                c['execution_count'] = None\n",
        "    return nb\n",
        "\n",
        "nb_clean = reset_metadata(nb_dict, MODE, STRIP_OUTPUTS)\n",
        "out_path = f\"/content/{name.rsplit('.ipynb',1)[0]}_reset.ipynb\"\n",
        "nbf.write(nbf.from_dict(nb_clean), out_path)\n",
        "print(\"Wrote:\", out_path)\n",
        "\n",
        "# 仕上げ確認（GitHubが落ちる元が残っていないか）\n",
        "print(\"Top-level widgets exists?:\", 'widgets' in nb_clean.get('metadata',{}))\n"
      ],
      "metadata": {
        "id": "Q_3So8snYYuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m2ZnKTndZErB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}